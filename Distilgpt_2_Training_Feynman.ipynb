{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Distilgpt-2 Training Feynman",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNd8SBmKmc7N"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91rmSAUQVIUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85c35cb6-07a3-4f6d-fe94-6fce2279e5b5"
      },
      "source": [
        "#Clone the transformers repo into the notebook\n",
        "!git clone https://github.com/huggingface/transformers"
      ],
      "execution_count": 346,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'transformers' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2M2Oz9CYB4P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9cf07a1-2736-4583-d942-a2ad5ee5b6a8"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "os.chdir(\"transformers\")\n",
        "os.chdir(\"./examples/language-modeling\")\n",
        "!ls"
      ],
      "execution_count": 347,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "README.md\t  run_clm.py\t   run_mlm_no_trainer.py  run_plm.py\n",
            "requirements.txt  run_mlm_flax.py  run_mlm.py\t\t  transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04rBGxwiYnep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4c0912d-0b2e-4fc6-e2bc-5ae89e9afa25"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 348,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets>=1.1.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.5.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.1.95)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (3.12.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.8.7)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.0.7)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (3.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (1.1.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.70.11.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->-r requirements.txt (line 3)) (54.2.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->-r requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets>=1.1.3->-r requirements.txt (line 1)) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets>=1.1.3->-r requirements.txt (line 1)) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets>=1.1.3->-r requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 1)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 1)) (2018.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eo5gRmXaWx0m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e034539b-7f51-4f23-95e4-06be54de5965"
      },
      "source": [
        "!pip install pyarrow --upgrade"
      ],
      "execution_count": 349,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: pyarrow in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pyarrow) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5sdYSpAWY1S"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/transformers/examples/\")\n",
        "os.chdir(\"./language-modeling\")"
      ],
      "execution_count": 350,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ACN6A-mUCal"
      },
      "source": [
        "cuda0 = torch.device('cuda:0')"
      ],
      "execution_count": 351,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6G6WINaYmwx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ddd8938-1564-4ecf-e925-2bb089a33a71"
      },
      "source": [
        "# Need to install latest transformer packages from github so the scripts will run correctly\n",
        "! pip install git+git://github.com/huggingface/transformers/"
      ],
      "execution_count": 352,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/huggingface/transformers/\n",
            "  Cloning git://github.com/huggingface/transformers/ to /tmp/pip-req-build-n6za6zu6\n",
            "  Running command git clone -q git://github.com/huggingface/transformers/ /tmp/pip-req-build-n6za6zu6\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied (use --upgrade to upgrade): transformers==4.5.0.dev0 from git+git://github.com/huggingface/transformers/ in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (3.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (0.0.44)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0.dev0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0.dev0) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0.dev0) (7.1.2)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.5.0.dev0-cp37-none-any.whl size=2064484 sha256=c36e8437b8cbe7a215d3a36f45b01f1122ff742520a7a385dcd532d732657cdf\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-g5es_pdv/wheels/dc/e5/1e/3a2977a646558fca07585cadcf56aa4a910e995ba945961c4e\n",
            "Successfully built transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDuJnVmrY_Fb"
      },
      "source": [
        "#Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uU4ckPTf9T-2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37f8203f-827f-40cd-b7cb-5a19fe919d37"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "with open('/content/Lectures_1.csv', 'r') as data:\n",
        "  dataset = [x.strip() for x in data.readlines()]\n",
        "\n",
        "  \n",
        "train, eval = train_test_split(dataset, train_size=.9, random_state=2020)\n",
        "print(\"training size:\" + str(len(train)))\n",
        "print(\"Evaluation size: \" + str(len(eval)))\n",
        "\n",
        "with open('/content/train_tmp.txt', 'w') as file_handle:\n",
        "  file_handle.write(\"<|endoftext|>\".join(train))\n",
        "  file_handle.close()\n",
        "\n",
        "with open('/content/eval_tmp.txt', 'w') as file_handle:\n",
        "  file_handle.write(\"<|endoftext|>\".join(eval))"
      ],
      "execution_count": 373,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training size:80505\n",
            "Evaluation size: 8946\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TYi8Oqs6Npo"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyV_rTL3ZE_H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50a66e12-01a2-4637-cf11-0e2c75f5c972"
      },
      "source": [
        "!python run_clm.py \\\n",
        "--model_type gpt2  \\\n",
        "--model_name_or_path distilgpt2 \\\n",
        "--train_file \"/content/train_tmp.txt\" \\\n",
        "--do_train \\\n",
        "--validation_file \"/content/eval_tmp.txt\" \\\n",
        "--do_eval \\\n",
        "--per_gpu_train_batch_size 1 \\\n",
        "--save_steps -1 \\\n",
        "--num_train_epochs 5 \\\n",
        "--fp16 \\\n",
        "--output_dir=\"/content/test_model\""
      ],
      "execution_count": 376,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-04 04:38:28.687334: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "04/04/2021 04:38:30 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "04/04/2021 04:38:30 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/content/test_model, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Apr04_04-38-30_e529a484f387, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=-1, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/content/test_model, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=1, mp_parameters=)\n",
            "04/04/2021 04:38:30 - WARNING - datasets.builder -   Using custom data configuration default-b44ebcc26f179b41\n",
            "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-b44ebcc26f179b41/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-b44ebcc26f179b41/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n",
            "[INFO|configuration_utils.py:479] 2021-04-04 04:38:30,662 >> loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
            "[INFO|configuration_utils.py:515] 2021-04-04 04:38:30,663 >> Model config GPT2Config {\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.5.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:479] 2021-04-04 04:38:30,711 >> loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
            "[INFO|configuration_utils.py:515] 2021-04-04 04:38:30,712 >> Model config GPT2Config {\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.5.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1707] 2021-04-04 04:38:31,026 >> loading file https://huggingface.co/distilgpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1707] 2021-04-04 04:38:31,026 >> loading file https://huggingface.co/distilgpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1707] 2021-04-04 04:38:31,026 >> loading file https://huggingface.co/distilgpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/accb287b5a5396b2597382916b6cc939fdab1366e89475a92338d3971b3d02b7.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1707] 2021-04-04 04:38:31,026 >> loading file https://huggingface.co/distilgpt2/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1707] 2021-04-04 04:38:31,026 >> loading file https://huggingface.co/distilgpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1707] 2021-04-04 04:38:31,026 >> loading file https://huggingface.co/distilgpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|modeling_utils.py:1052] 2021-04-04 04:38:31,144 >> loading weights file https://huggingface.co/distilgpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba\n",
            "[INFO|modeling_utils.py:1168] 2021-04-04 04:38:34,986 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1177] 2021-04-04 04:38:34,986 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "[WARNING|tokenization_utils_base.py:3137] 2021-04-04 04:38:35,958 >> Token indices sequence length is longer than the specified maximum sequence length for this model (52579 > 1024). Running this sequence through the model will result in indexing errors\n",
            "100% 1/1 [00:03<00:00,  3.66s/ba]\n",
            "100% 1/1 [00:00<00:00,  2.51ba/s]\n",
            "100% 1/1 [00:01<00:00,  1.17s/ba]\n",
            "100% 1/1 [00:00<00:00,  7.97ba/s]\n",
            "[INFO|trainer.py:402] 2021-04-04 04:38:44,898 >> Using amp fp16 backend\n",
            "[WARNING|training_args.py:628] 2021-04-04 04:38:45,128 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:628] 2021-04-04 04:38:45,130 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|trainer.py:1013] 2021-04-04 04:38:45,130 >> ***** Running training *****\n",
            "[INFO|trainer.py:1014] 2021-04-04 04:38:45,130 >>   Num examples = 1251\n",
            "[INFO|trainer.py:1015] 2021-04-04 04:38:45,130 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1016] 2021-04-04 04:38:45,130 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1017] 2021-04-04 04:38:45,130 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:1018] 2021-04-04 04:38:45,131 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1019] 2021-04-04 04:38:45,131 >>   Total optimization steps = 6255\n",
            "[WARNING|training_args.py:628] 2021-04-04 04:38:45,135 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "  0% 0/6255 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "{'loss': 3.6681, 'learning_rate': 4.600319744204636e-05, 'epoch': 0.4}\n",
            "{'loss': 3.3802, 'learning_rate': 4.200639488409273e-05, 'epoch': 0.8}\n",
            "{'loss': 3.2427, 'learning_rate': 3.8009592326139094e-05, 'epoch': 1.2}\n",
            "{'loss': 3.1727, 'learning_rate': 3.401278976818545e-05, 'epoch': 1.6}\n",
            "{'loss': 3.141, 'learning_rate': 3.0015987210231815e-05, 'epoch': 2.0}\n",
            "{'loss': 3.0594, 'learning_rate': 2.6019184652278178e-05, 'epoch': 2.4}\n",
            "{'loss': 3.0426, 'learning_rate': 2.2022382094324544e-05, 'epoch': 2.8}\n",
            "{'loss': 3.0049, 'learning_rate': 1.8025579536370903e-05, 'epoch': 3.2}\n",
            "{'loss': 2.9803, 'learning_rate': 1.4028776978417266e-05, 'epoch': 3.6}\n",
            "{'loss': 2.9795, 'learning_rate': 1.003197442046363e-05, 'epoch': 4.0}\n",
            "{'loss': 2.9459, 'learning_rate': 6.035171862509993e-06, 'epoch': 4.4}\n",
            "{'loss': 2.9391, 'learning_rate': 2.0383693045563552e-06, 'epoch': 4.8}\n",
            "100% 6255/6255 [19:14<00:00,  5.47it/s][INFO|trainer.py:1196] 2021-04-04 04:57:59,460 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1154.3292, 'train_samples_per_second': 5.419, 'epoch': 5.0}\n",
            "100% 6255/6255 [19:14<00:00,  5.42it/s]\n",
            "[INFO|trainer.py:1648] 2021-04-04 04:57:59,652 >> Saving model checkpoint to /content/test_model\n",
            "[INFO|configuration_utils.py:318] 2021-04-04 04:57:59,653 >> Configuration saved in /content/test_model/config.json\n",
            "[INFO|modeling_utils.py:831] 2021-04-04 04:58:00,442 >> Model weights saved in /content/test_model/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1901] 2021-04-04 04:58:00,442 >> tokenizer config file saved in /content/test_model/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1907] 2021-04-04 04:58:00,443 >> Special tokens file saved in /content/test_model/special_tokens_map.json\n",
            "[INFO|trainer_pt_utils.py:735] 2021-04-04 04:58:00,526 >> ***** train metrics *****\n",
            "[INFO|trainer_pt_utils.py:740] 2021-04-04 04:58:00,526 >>   epoch                      =        5.0\n",
            "[INFO|trainer_pt_utils.py:740] 2021-04-04 04:58:00,526 >>   init_mem_cpu_alloc_delta   =     1377MB\n",
            "[INFO|trainer_pt_utils.py:740] 2021-04-04 04:58:00,526 >>   init_mem_cpu_peaked_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:740] 2021-04-04 04:58:00,527 >>   init_mem_gpu_alloc_delta   =      319MB\n",
            "[INFO|trainer_pt_utils.py:740] 2021-04-04 04:58:00,527 >>   init_mem_gpu_peaked_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:740] 2021-04-04 04:58:00,527 >>   train_mem_cpu_alloc_delta  =       29MB\n",
            "[INFO|trainer_pt_utils.py:740] 2021-04-04 04:58:00,527 >>   train_mem_cpu_peaked_delta =        0MB\n",
            "[INFO|trainer_pt_utils.py:740] 2021-04-04 04:58:00,527 >>   train_mem_gpu_alloc_delta  =      941MB\n",
            "[INFO|trainer_pt_utils.py:740] 2021-04-04 04:58:00,527 >>   train_mem_gpu_peaked_delta =     1942MB\n",
            "[INFO|trainer_pt_utils.py:740] 2021-04-04 04:58:00,527 >>   train_runtime              = 0:19:14.32\n",
            "[INFO|trainer_pt_utils.py:740] 2021-04-04 04:58:00,527 >>   train_samples              =       1251\n",
            "[INFO|trainer_pt_utils.py:740] 2021-04-04 04:58:00,527 >>   train_samples_per_second   =      5.419\n",
            "04/04/2021 04:58:00 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:1865] 2021-04-04 04:58:00,634 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1866] 2021-04-04 04:58:00,634 >>   Num examples = 140\n",
            "[INFO|trainer.py:1867] 2021-04-04 04:58:00,634 >>   Batch size = 8\n",
            "100% 18/18 [00:05<00:00,  3.40it/s]\n",
            "[INFO|trainer_pt_utils.py:735] 2021-04-04 04:58:06,288 >> ***** eval metrics *****\n",
            "[INFO|trainer_pt_utils.py:740] 2021-04-04 04:58:06,288 >>   epoch                     =        5.0\n",
            "[INFO|trainer_pt_utils.py:740] 2021-04-04 04:58:06,288 >>   eval_loss                 =     2.9979\n",
            "[INFO|trainer_pt_utils.py:740] 2021-04-04 04:58:06,288 >>   eval_mem_cpu_alloc_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:740] 2021-04-04 04:58:06,288 >>   eval_mem_cpu_peaked_delta =        0MB\n",
            "[INFO|trainer_pt_utils.py:740] 2021-04-04 04:58:06,288 >>   eval_mem_gpu_alloc_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:740] 2021-04-04 04:58:06,288 >>   eval_mem_gpu_peaked_delta =     5164MB\n",
            "[INFO|trainer_pt_utils.py:740] 2021-04-04 04:58:06,288 >>   eval_runtime              = 0:00:05.50\n",
            "[INFO|trainer_pt_utils.py:740] 2021-04-04 04:58:06,288 >>   eval_samples              =        140\n",
            "[INFO|trainer_pt_utils.py:740] 2021-04-04 04:58:06,289 >>   eval_samples_per_second   =     25.446\n",
            "[INFO|trainer_pt_utils.py:740] 2021-04-04 04:58:06,289 >>   perplexity                =    20.0426\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFOx9AUa1tnk"
      },
      "source": [
        "from transformers import GPT2LMHeadModel\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"/content/test_model\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n"
      ],
      "execution_count": 378,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZ3YNTNlBsh8"
      },
      "source": [
        "#Generate Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xT0tc07_-SL"
      },
      "source": [
        "input_ids = tokenizer.encode(\"The reason\", return_tensors='tf')"
      ],
      "execution_count": 379,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxtUSIAc_-1G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b6cc399-539d-43d8-f9e9-b2953cf09fc7"
      },
      "source": [
        "# the tf tensor object\n",
        "input_ids[0]"
      ],
      "execution_count": 380,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([ 464, 1738], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 380
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbzHNvvaAPns",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "3d8bccbf-0873-4592-bc81-89faa4fbe5dc"
      },
      "source": [
        "generated_text_samples = model.generate(\n",
        "    input_ids, \n",
        "    max_length=200,  \n",
        "    num_return_sequences=5,\n",
        "    temperature=.95,\n",
        "    do_sample=True,\n",
        "    top_k=100,\n",
        ")"
      ],
      "execution_count": 381,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-381-e789354218fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.95\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, **model_kwargs)\u001b[0m\n\u001b[1;32m    912\u001b[0m             \u001b[0;31m# init `attention_mask` depending on `pad_token_id`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m             model_kwargs[\"attention_mask\"] = self._prepare_attention_mask_for_generation(\n\u001b[0;32m--> 914\u001b[0;31m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_token_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meos_token_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m             )\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36m_prepare_attention_mask_for_generation\u001b[0;34m(self, input_ids, pad_token_id, eos_token_id)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_pad_token_in_inputs_ids\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_pad_token_not_equal_to_eos_token_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_ones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m     def _prepare_encoder_decoder_kwargs_for_generation(\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'new_ones'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOj5vPM2AY-f",
        "outputId": "02121e06-7bf2-43ac-cf25-b47a9b5a7bc6"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"/content/test_model\")\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "model = model.to('cuda')\n",
        "\n",
        "def generate(input_str, length=250, n=5):\n",
        "  cur_ids = torch.tensor(tokenizer.encode(input_str)).unsqueeze(0).long().to(device)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for i in range(length):\n",
        "      outputs = model(cur_ids[:, -1024:], labels=cur_ids[:, -1024:])\n",
        "      loss, logits = outputs[:2]\n",
        "      softmax_logits = torch.softmax(logits[0,-1], dim=0)\n",
        "      next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=n)\n",
        "      cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim=1)\n",
        "    output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
        "    output_text = tokenizer.decode(output_list)\n",
        "    return output_text\n",
        "\n",
        "def choose_from_top(probs, n=5):\n",
        "    ind = np.argpartition(probs, -n)[-n:]\n",
        "    top_prob = probs[ind]\n",
        "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
        "    choice = np.random.choice(n, 1, p = top_prob)\n",
        "    token_id = ind[choice][0]\n",
        "    return int(token_id)\n",
        "\n",
        "generated_text = generate(\" = Physics = \\n\")\n",
        "print(generated_text)"
      ],
      "execution_count": 377,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " = Physics = \n",
            "\n",
            "{\n",
            "\n",
            "// This is a very simple function to use, just use the function\n",
            "\n",
            "// in the constructor\n",
            "\n",
            "if (! is_null ())\n",
            "\n",
            "{\n",
            "\n",
            "return nullptr ;\n",
            "\n",
            "}\n",
            "\n",
            "// This is a very simple function to call in the constructor\n",
            "\n",
            "if (! is_null () &&! is_null () )\n",
            "\n",
            "{\n",
            "\n",
            "return nullptr ;\n",
            "\n",
            "}\n",
            "\n",
            "// This is a very simple function to call in the constructor\n",
            "\n",
            "if (! is_null () &&! is_null (). empty ())\n",
            "\n",
            "{\n",
            "\n",
            "return nullptr ;\n",
            "\n",
            "}\n",
            "\n",
            "return is_null ()? nullptr : nullptr ;\n",
            "\n",
            "}\n",
            "\n",
            "// This is a function to call in the constructor\n",
            "\n",
            "// in a function\n",
            "\n",
            "if (! is_null () && is_null () && is_null () && is_null ( is_null (), nullptr ))\n",
            "\n",
            "{\n",
            "\n",
            "return nullptr ;\n",
            "\n",
            "}\n",
            "\n",
            "// This is a function to call in the constructor\n",
            "\n",
            "// in an object\n",
            "\n",
            "if (! is_null ())\n",
            "\n",
            "{\n",
            "\n",
            "return nullptr ;\n",
            "\n",
            "}\n",
            "\n",
            "else\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPdteSR_B3w1"
      },
      "source": [
        "# Print output for each sequence generated above\n",
        "for i, beam in enumerate(generated_text_samples):\n",
        "  print(\"{}: {}\".format(i,tokenizer.decode(beam, skip_special_tokens=True)))\n",
        "  print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cKzLkoGt1ku"
      },
      "source": [
        "!zip -r /content/file.zip /content/Folder_To_Zip\n",
        "from google.colab import files\n",
        "files.download(\"/content/file.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ono1vYGEwSRQ"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}